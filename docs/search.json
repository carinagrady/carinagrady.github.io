[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "Engineer. Analyst. Community Builder. Creative Mind.\nI’ve always been someone who finds patterns, whether in numbers, cities, or the ways people interact. My background is in engineering and data analytics, which means I spend a lot of time making sense of complex systems and turning data into insights. I’ve worked with national labs, energy consultants, and non-profit initiatives—all with the goal of making communities stronger and more sustainable. I love taking complex information, making sense of it, and using it to drive real change.\nBut life isn’t just about logic and structure; creativity and human connection matter just as much to me. Outside of spreadsheets and strategy sessions, I’m dancing, writing, reading, and highly engaged with my local community. I love how art, music, and storytelling bring people together, and I try to infuse that same spirit of collaboration into everything I do—whether it’s working on a community project, mentoring others in data analysis, or simply sharing a great conversation. I believe the strongest communities are built at the intersection of different disciplines, where data meets storytelling, and structure meets spontaneity.\nThis site is a mix of everything I love—data, creativity, and community. Feel free to look around, and if you see something that sparks an idea, let’s connect!\n\n\n\nAlso a fanatic alpaca enthusiast, featuring my favorite picture in Peru!"
  },
  {
    "objectID": "portfolio-community.html",
    "href": "portfolio-community.html",
    "title": "Community Engagement Portfolio",
    "section": "",
    "text": "Below are some of my research notes, opinion pieces, and volunteer work, mostly related to environmental sustainability.\n\nHydroponics Project\nHydroponics Project - A hands-on project focused on hydroponic farming to address food insecurity. Our team designed a low-cost hydroponic system that can grow fresh produce year-round. The initiative partnered with seven community organizations and impacted over 2,000 people, mostly affecting children."
  },
  {
    "objectID": "portfolio-engineering.html",
    "href": "portfolio-engineering.html",
    "title": "Engineering Portfolio",
    "section": "",
    "text": "Below are some of my past engineering-related projects, publications, and consultant work from clients, national labs, and internships.\n\n\nConsulting\n\nEvaluation, Measurement & Verification (EM&V) Overview\nEM&V Overview - An introduction to the principles of EM&V, which ensures that energy efficiency programs achieve their intended results. While my client-based EM&V work is proprietary, I regularly conduct impact assessments, data analysis, and reporting to validate energy savings for utilities.\n\nHeat Pump Metering Study\nHeat Pump Metering Study - A comprehensive study on heat pump efficiency in Massachusetts and Connecticut during winter months. I supported the analytics team by processing large datasets to assess seasonal performance and survey responses. \nNational Residential Efficiency Measures Database (REMD)\nNational Residential Efficiency Measures Database (REMD) - As part of a team updating the REMD dataset, I helped scrape retail energy pricing data and develop regression models to predict pricing trends for modelers and researchers.\n\n\n\n\nResearch & Publications\n\nHydrogen Storage System Design Tool\nHydrogen Publication - My first-author publication on a computational tool for designing adsorbent hydrogen storage systems in fuel cell vehicles. The tool integrates material properties, thermodynamic equations, and system-level modeling to optimize hydrogen storage performance. \nPressure Plug Patent Request\nPressure Plug Patent Request - A patent submission for a low-profile, high-pressure hydrogen tank plug designed for submarine applications. I developed a CAD model for manufacturing and assisted in creating a test plan to validate performance under high pressure.\n\nNational Lab Research\n\nA few images from my time at the national lab. I worked on flow cell batteries in glove boxes and explored novel 3D printing techniques. The syringe extruder shown was used for preliminary testing of wet carbon fiber-reinforced cement, with frosting and toothpaste serving as test materials for easy cleanup. (It also makes for a fun story.)\n\n\n\n\nInternships\n\nInternship at Nuclear Power Plant\n\nA collage from my time interning at California’s only nuclear power plant, where I gained hands-on experience in power generation and system operations.\nNuclear Power Plant Systems Drawing\n\nA schematic I developed during my internship to illustrate the power generation cycle. I found it quite useful for understanding thermodynamic principles in action.\nInternship for Mechanical Design\n\nThis Canada-based internship focused on mechanical design for railway maintenance equipment. One of the more amusing moments was a company-wide gathering where we formed a maple leaf, wore red noses, and received bottles of maple syrup as gifts.\n\n\n\n\nEngineering & Technical Reports\nBelow are some of my past projects and reports from my time at Iowa State University (B.S. Mechanical Engineering) and Georgia Institute of Technology (partial M.S. Analytics), categorized by subject area.\n\nFluids Lab Report - Experimental analysis of fluid dynamics principles in a controlled environment.\nHeat Transfer Lab Report - Investigating heat transfer efficiency in different materials and conditions.\nPumped Hydro Storage Thesis Report - A research thesis on energy storage solutions using pumped hydro.\nElectric Vehicles Report - Analysis of electric vehicle technology, performance, and efficiency.\nPower Generation Report - Technical insights into modern power generation systems.\nSenior Capstone Poster - Final-year project poster summarizing key findings and innovations.\nSolarShack Project - A conceptual design for a sustainable, solar-powered housing solution.\n\n\n\nAnalytics & Data Science\n\nBusiness Analytics Report - An in-depth business intelligence analysis using statistical and machine learning methods.\nRegression Part 1 - Linear regression modeling and interpretation.\nRegression Part 2 2 - Regression assumptions and diagnostics.\nRegression Part 3 - Logistical regression and interpretation.\nRegression Part 4 - Advanced regression and feature selection.\nAnalytics Notes - A compilation of key analytics concepts and methodologies.\nPython Evaluation - A web-based Python assignment showcasing coding principles.\n\n\n\nVisual & Interactive Work\n\nISU Infographics - A collection of visual data storytelling projects.\nPython Animation 1 - A dynamic animation generated using Python.\nPython Animation 2 - Another Python-based visualization."
  },
  {
    "objectID": "second-brain.html",
    "href": "second-brain.html",
    "title": "Second Brain",
    "section": "",
    "text": "I’ve always been drawn to the new and different: the ideas that push boundaries, challenge assumptions, and redefine what’s possible. Whether it’s uncovering patterns in data, questioning long-held beliefs in technology, or exploring the social forces that shape our world, I love the process of discovery. I’m drawn to understanding how things work—not just taking them at face value, but exploring different perspectives and possibilities to see what else might be there.\nThat’s why I find myself at the intersection of engineering, analytics, and the humanities. I know it might not seem like the most obvious combination (after all, there is left brain and right brain; numbers and logic on one side, stories and human experience on the other), but to me, they’re deeply connected.\n\n\n\n\n\nEngineering and analytics give us the tools to build and understand complex systems, from energy grids to economic models. But the humanities help us ask the right questions. They remind us that technology doesn’t exist in a vacuum; it shapes and is shaped by people, culture, and history. To truly innovate, we need both—the precision of data and the depth of human insight.\nData can reveal social truths that impacts every individual, engineered solutions can be more effective when they consider human behavior, and storytelling and history can influence innovation. The world is built on connections between people, ideas, and disciplines. For me, the best way to make an impact is by embracing that interconnectedness, by considering what happens when we bring these fields together. This consideration fuels my curiosity, my work, and my passion for exploring the bigger picture."
  },
  {
    "objectID": "second-brain.html#systems-thinking",
    "href": "second-brain.html#systems-thinking",
    "title": "Second Brain",
    "section": "Systems Thinking",
    "text": "Systems Thinking\nEngineers solve problems by designing systems. Whether it’s infrastructure, software, energy grids, or manufacturing, real-world engineering challenges aren’t isolated. They exist within immensely complex, interconnected systems, where one change can influence everything else.\nSystems thinking is a way of making sense of this complexity. Instead of just focusing on individual components, engineers look at how things interact, how information flows, and how small adjustments ripple through the entire system. Instead of focusing on individual parts, it’s about stepping back and looking at the bigger picture. Engineers ask:\n\nWhat are the key parts of this system?\nHow do they influence each other?\nWhat happens if something changes?\nHow can we optimize for efficiency, stability, or adaptability?\n\nMost engineering systems can be broken down into:\n\n\n\n\n\n\n\n\nComponent\nDefinition\nExample\n\n\n\n\nSystem\nThe entire structure with all its interconnected parts\nElectrical grid\n\n\nSubsystem\nA functional unit within the larger system\nPower generation, transmission, distribution\n\n\nUnit Type\nA specific element within a subsystem\nTurbine, transformer, solar panel\n\n\nOperations\nThe processes that keep the system running\nEnergy flow, load balancing, grid frequency regulation\n\n\n\nUnderstanding systems isn’t just about making things more efficient; it’s about making them work better as a whole, long-term. Without a systems mindset, solutions can be short-sighted, solving one issue while creating several new ones. Systems thinking prevents this by keeping the bigger picture in focus. In an increasingly complex world, the ability to step back, recognize patterns, and optimize for the bigger picture is what separates quick fixes from long-lasting solutions."
  },
  {
    "objectID": "second-brain.html#energy-systems",
    "href": "second-brain.html#energy-systems",
    "title": "Second Brain",
    "section": "Energy Systems",
    "text": "Energy Systems\nEnergy is everywhere. It moves our bodies, fuels our machines, and powers our cities. But energy itself doesn’t appear or disappear—it transforms. This is the foundation of thermodynamics, the study of how energy moves and changes forms.\nAt its core is the conservation of energy, which states that energy cannot be created or destroyed, only converted. Whether in a boiling pot of water, a moving car, or a power plant, the same principle applies:\n\\[\n\\Delta E = Q - W\n\\] Expanding for all forms of energy:\n\\[\n\\Delta U + \\Delta KE + \\Delta PE = Q - W + m \\left( \\frac{P}{\\rho} \\right)\n\\]\nWhere:\n\n\\(U\\) = Internal energy (heat stored inside an object)\n\\(KE\\) = Kinetic energy (energy from movement)\n\\(PE\\) = Potential energy (energy due to position)\n\\(Q\\) = Heat transfer (energy added or removed)\n\\(W\\) = Work done (energy used to move something)\n\\(m\\) = Mass (amount of matter in the system)\n\\(P\\), \\(\\rho\\) = Fluid pressure and density (energy in flowing fluids)\n\nThese equation may look merely theoretical, but they are the blueprint for modern civilization. By understanding how energy moves, engineers have designed systems that harness it efficiently, from the simplest engines to the most advanced power grids. In fact, humans have used energy transformation to our advantage for thousands of years:\n\nFire (Chemical → Thermal Energy) – Early civilizations burned wood to cook, stay warm, and forge metal.\nSteam Engines (Chemical → Thermal → Mechanical Energy) – In the 1700s, steam engines transformed heat from burning coal into movement, powering trains and industry.\nPower Plants (Chemical → Thermal → Mechanical → Electrical Energy) – The industrial revolution introduced power plants, scaling up energy conversion to produce electricity for entire cities. Common fuels used include coal, natural gas, and uranium.\nRenewables (Wind, Solar, Hydro) – Today, we’re transitioning to energy sources that convert natural forces directly into electricity—wind turbines (Kinetic → Electrical), solar photovoltaics (Light → Electrical), and hydroelectric dams (Mechanical → Electrical Energy).\n\nEnergy is the foundation of civilization. From early fire to modern power grids, the ability to transform and distribute it efficiently has shaped history. As technology advances, the way we generate and manage energy will define the future. Improving efficiency, integrating renewables, and designing smarter grids are challenges that will impact sustainability, industry, and daily life, making energy an important and exciting fields to explore."
  },
  {
    "objectID": "second-brain.html#electrical-grid",
    "href": "second-brain.html#electrical-grid",
    "title": "Second Brain",
    "section": "Electrical Grid",
    "text": "Electrical Grid\nAccording to the National Academy of Engineering, the number one greatest engineering achievement of the 20th century is electrification. It has enabled modern civilization, from lighting cities to powering industries, transforming nearly every aspect of daily life. Yet, over a century after its invention, the grid still operates on a rigid, century-old framework: power plants generate electricity, transmission lines carry it vast distances, and distribution networks deliver it to homes and businesses. This system has worked remarkably well—but it was built for a different era, one without widespread renewable energy or real-time analytics.\nThe challenge today is not only to generate enough electricity, but distributing it efficiently in a rapidly changing world. Renewable energy sources like wind and solar introduce variability, requiring new approaches to maintaining a stable power supply. Climate change adds further urgency, demanding that we rethink how electricity is generated, stored, and delivered.\nAt its core, the grid is a network of interconnected subsystems, each with a crucial role:\n\nPower Generation – Energy is produced through coal, natural gas, nuclear, hydro, wind, and solar power.\nTransmission – High-voltage lines move electricity efficiently across long distances.\nDistribution – Local networks step down the voltage and deliver power to homes and businesses.\n\nFor over a century, these components have worked together to provide reliable electricity. But integrating intermittent renewables disrupts this balance, introducing complexities that traditional grid models weren’t designed for. The grid must match electricity supply with demand at every moment, but not all power sources operate the same way:\n\nBaseline power – Consistently running power plants, such as nuclear and coal, provide a steady supply of electricity.\nPeaker plants – Gas-powered plants can ramp up quickly during high demand but are expensive and inefficient to run frequently.\nIntermittent renewables– Solar and wind produce energy when nature allows, which doesn’t always align with demand.\n\nHistorically, baseline plants handled the majority of energy needs, while peaker plants filled in gaps during peak hours. But as renewables become more common, they disrupt this balance. Without better energy storage or grid flexibility, excess solar energy at noon can go to waste, while evening peaks still rely on fossil fuels.\n\n\n\nConsider the duck curve, which illustrates the challenge of solar energy integration. Midday solar generation reduces demand for traditional power plants, but as the sun sets, demand surges, requiring a steep ramp-up in generation from peaker plants. To flatten the duck, we need better energy storage, demand shifting, and grid flexibility. Otherwise, we’re left with an unstable (but appropriately named) energy challenge.\n\n\nRenewable energy generation can lead to overproduction when demand is low and shortages when demand is high. To manage this, grid operators must either curtail excess energy (wasting it) or store it for later use (which is currently limited). The goal is to flatten the duck curve by spreading out electricity demand and improving storage solutions. In other words, we need to make the grid a little less duck-shaped—because while ducks are great in ponds, they make for a pretty unstable energy system.\nA quick note on supply and demand. Every year, our demand for electricity increases (i.e. more data centers, more electric vehicles, more air conditioning), but power generation remains relatively constant. This may seem counterintuitive, but it’s largely due to energy efficiency improvements. LED lighting, smarter appliances, and improved industrial processes allow us to do more with the same amount of energy. However, even with these gains, the shift to electrified transportation and digital infrastructure means we must rethink how we scale energy production.\nTomorrow’s grid must be smarter, more flexible, and data-driven. Engineers and analysts are developing solutions such as:\n\nBattery storage – Capturing excess renewable energy for later use.\nDecentralized microgrids – Local energy networks that improve resilience.\nReal-time analytics – Predicting demand fluctuations and optimizing power flow.\nAutomated grid management – AI-driven decision-making for efficiency.\n\nThe grid may look only like infraastructure, but it is a complex dynamic system that shapes economies and societies. Transforming the grid requires balancing reliability, affordability, and sustainability. Simulating future grid scenarios with data analytics will help ensure a system that works for everyone and defines the future."
  },
  {
    "objectID": "second-brain.html#exploratory-data-analysis",
    "href": "second-brain.html#exploratory-data-analysis",
    "title": "Second Brain",
    "section": "Exploratory Data Analysis",
    "text": "Exploratory Data Analysis\nBefore building any models, data must be explored, cleaned, and understood. Exploratory Data Analysis (EDA) is the process of investigating raw data, identifying patterns, and preparing it for meaningful analysis. The reality is that the majority of data science is not about advanced modeling at all; rather, it’s about making sense of messy, real-world data.\nAs John Tukey famously said, “Embrace your data, not your models.” A model is only as good as the data feeding it. Without a deep understanding of the data itself, even the most sophisticated model will be misleading at best and completely wrong at worst.\nIn a perfect world, data would be pristine: consistent, complete, and ready for analysis. Unfortunately, real-world data rarely looks like that. Instead, it comes from multiple sources, gets entered manually, is often missing key details, and sometimes outright wrong. Consider a few real-life challenges:\n\nInconsistent Formatting – A name might appear as \"J. DOE\", \"JOHN DOE\", \"john doe\", or \"John A. Doe\", making identity matching difficult.\nEntry Errors – A missing decimal could turn $100.00 into $10,000.00, creating a major outlier.\nConflicting Date Formats – \"1 January 1970\" and \"01/01/1970\" are the same date but require standardization.\nRecords Discrepancies – A hospital’s patient records may vary depending on how different doctors enter information.\nSurvey Gaps – Respondents may skip questions, leading to missing values.\nSensor Malfunctions – Weather stations sometimes record impossible temperatures due to hardware failures.\n\nMessy data is everywhere, but it is where data science begins. A robust, well-documented, and reproducible EDA process is the foundation of reliable analysis. Without it, conclusions are flawed, models are unreliable, and insights become meaningless. Understanding what’s missing, what’s misleading, and what might introduce bias is critical before drawing any conclusions.\nOne of the quickest ways to summarize and explore data is through pivot tables. They allow analysts to dynamically slice and dice information across different categories, compute aggregates (like sums, averages, and counts), and uncover hidden trends.\nBelow is an interactive pivot table using rpivotTable that breaks down CO₂ emissions by vehicle type. Try selecting different attributes like MPG or weight to see how they relate.\n\n\n\n\n\n\n Numbers in a table can provide insight, but visualization is an intuitiive way to explore data. Charts and graphs help reveal trends, outliers, and relationships in ways that raw numbers can’t. A few common techniques include boxplots, histograms, and scatterplots.\nThe boxplot below provides a clear visualization of the same CO₂ emission data from the pivot table.\n\n\n\n\n\n\n\n\n\nIntuitively, the result makes sense: the bigger the vehicle, the higher the emissions. But how strong is this relationship? Is weight the only factor? To understand this better, we need to examine correlation.\nOne of the biggest pitfalls in analysis is mistaking correlation for causation. Correlation does not result in causation; just because two variables move together doesn’t mean one is causing the other. Without deeper investigation, data can tell a misleading story that might seem logical at first but falls apart with further scrutiny. The classic example of ice cream sales and shark attacks illustrates this perfectly.\n\n\n\nAt first glance, the pattern is clear: more ice cream sales, more shark attacks. So should we ban ice cream to save lives? Not quite. The real factor at play is hot weather, since it sends people to the beach (where sharks happen to be) and also increases ice cream consumption. The two trends move together, but one isn’t causing the other.\n\n\nBelow is a correlation heatmap of our dataset, revealing the key relationships between vehicle attributes. CO₂ emissions strongly correlate with MPG, model year, and weight. This intuitively makes sense; fuel-efficient cars emit less CO₂, newer models tend to be cleaner, and heavier vehicles produce more emissions.\n\n\n\n\n\n\n\n\n\nRemember, EDA is not optional—it is the most important step in any data analysis. Without it, models are built on assumptions rather than reality. By thoroughly exploring data, we can identify patterns, question biases, and refine our approach before jumping to conclusions. A great model can’t fix bad data, but great EDA can prevent bad models."
  },
  {
    "objectID": "second-brain.html#types-of-models",
    "href": "second-brain.html#types-of-models",
    "title": "Second Brain",
    "section": "Types of Models",
    "text": "Types of Models\nThe table below breaks down different modeling approaches, from supervised and unsupervised learning to time-series analysis and probability-based methods. These categories help define how data is used, whether it’s predicting an outcome, grouping similar observations, or analyzing uncertainty.\n\n\n\n\n\n\n\n\nCategory\nModels & Techniques\nPurpose\n\n\n\n\nSupervised Learning\nClassificaiton (SVM, KNN), Regression (Linear, Logistic, Advanced), Decision Trees (CART, Random Forests), Neural Networks (Deep Learning)\nUses labeled data to predict outcomes.\n\n\nUnsupervised Learning\nClustering (k-Means, DBSCAN), Dimensionality Reduction (PCA)\nFinds patterns in unlabeled data.\n\n\nTime-Series Models\nForecasting (ARIMA, GARCH), Trend Analysis (Exponential Smoothing), Change Detection (CUSUM)\nAnalyzes temporal dependencies for trend prediction.\n\n\nProbability-Based Models\nDistribution Fitting, A/B Testing, Markov Chains, Bayesian Statistics, Simulation\nModels uncertainty and probabilistic relationships.\n\n\n\nThis may seem like an abstract technical concepts, but models like these actually help us make sense of the world. In a way, they’re not so different from how we think; after all, our brains function like neural networks, constantly learning from past experiences (our “training data”, if you will) and adjusting based on new information. We don’t need massive datasets or code to do it, but the process is familiar: we take in patterns, form expectations, and refine our judgment."
  },
  {
    "objectID": "second-brain.html#modeling-process",
    "href": "second-brain.html#modeling-process",
    "title": "Second Brain",
    "section": "Modeling Process",
    "text": "Modeling Process\nThe second table below walks through the full data modeling pipeline, from preparing raw data to engineering meaningful features, applying descriptive, predictive, or prescriptive models, and finally ensuring the model is validated and deployable. Each step plays a role in making data-driven decisions more accurate, reliable, and actionable.\n\n\n\n\n\n\n\n\nStage\nKey Concepts\nPurpose\n\n\n\n\nPre-Modeling (Data Preparation)\nOutlier Detection, Data Cleaning, Transformations, Scaling, Imputation\nEnsures clean, high-quality data before modeling.\n\n\nFeature Engineering\nVariable Selection, Principal Component Analysis (PCA)\nReduces dimensionality and improves model performance.\n\n\nDescriptive Models\nSummary Statistics, Data Visualization, Clustering\nIdentifies patterns, trends, and structure in data.\n\n\nPredictive Models\nSupervised Learning, Time-Series Forecasting\nFinds hidden relationships and forecasts future trends.\n\n\nPrescriptive Models\nOptimization, Simulation, Game Theory, Reinforcement Learning\nRecommends actions to maximize desired outcomes.\n\n\nPost-Modeling (Deployment)\nCross-validation, Model Evaluation, Bias Detection, Interpretability\nEnsures reliability, fairness, and usability of models.\n\n\n\nIn many ways, this process mirrors how we reason and navigate the world. We:\n\nGather information first: just like pre-modeling data preparation.\nFind patterns: similar to descriptive models identifying trends.\nMake predictions: drawing from past experiences, much like predictive models.\nMake decisions: choosing the best action, just like prescriptive models.\n\nWe start by gathering information (pre-modeling), focus on what matters (feature engineering), recognize patterns (descriptive models), anticipate outcomes (predictive models), and, when possible, make decisions to optimize results (prescriptive models). Then we deploy our model by communicating to others—though whether that “model” is fair, accurate, or completely overfitted to our own biases is a whole other discussion (and probably a debate waiting to happen).\nUnderstanding this structure helps clarify when and why different techniques are used. Not every project needs all three modeling types (descriptive, predictive, and prescriptive), but seeing how they interact allows for better problem-solving and decision-making.\nAnd just like AI, we’re constantly learning, adapting, refining, and making decisions based on experience. Whether we’re analyzing data, forecasting the future, or optimizing outcomes, these models are shaping technology and reflect how we think, reason, and make sense of the world."
  },
  {
    "objectID": "second-brain.html#regression",
    "href": "second-brain.html#regression",
    "title": "Second Brain",
    "section": "Regression",
    "text": "Regression\nRegression is one of the most fundamental and widely used tools in analytics, and for good reason. At its core, regression helps us quantify relationships between variables, making it a go-to method in research, economics, engineering, and even everyday decision-making. It’s also incredibly intuitive—we recognize patterns constantly in our daily lives. For example, we know that more study hours tend to result in better grades, increased exercise often leads to improved health, and a car’s fuel efficiency correlates with its CO₂ emissions. Regression formalizes these observations into a mathematical framework to predict future outcomes.\nDespite its power, regression isn’t infallible. It relies on key assumptions:\n\n\n\n\n\n\n\n\n\nAssumption\nDefinition\nDiagnostic Tool\nResolution\n\n\n\n\nLinearity\nThe relationship between the independent and dependent variables should be linear.\nScatter plots, Residual vs. Fitted plots\nApply logarithmic, square root, or polynomial transformations.\n\n\nHomoscedasticity\nThe variance of residuals should remain constant across all levels of the independent variable(s).\nResidual vs. Fitted plot\nUse Box-Cox transformation (on response variable), log transformation, or weighted least squares.\n\n\nNormality of Residuals\nThe residuals should be normally distributed for valid hypothesis testing.\nQ-Q Plots, Histograms\nApply transformations (log, Box-Cox) to the dependent variable. Only evaluate normality using residuals, not the response variable.\n\n\nIndependence of Errors\nObservations should not be correlated over time (no autocorrelation).\nDurbin-Watson test, Residual vs. Time plot\nIf data is from a randomized trial, independence is likely established. If from observational studies, check for uncorrelated errors rather than independent errors.\n\n\nNo Multicollinearity\nIndependent variables should not be highly correlated with each other.\nVariance Inflation Factor (VIF), Correlation matrix\nRemove/combine highly correlated variables, use PCA, or use lasso, ridge, or elastic net regression.\n\n\nNo Outliers\nOutliers and high-leverage points should not disproportionately impact the regression model.\nCook’s Distance, Leverage plots, Box plots\nCorrect or remove.\n\n\nAdequate Sample Size\nEnsures sufficient observations per predictor variable to avoid overfitting.\nRule of thumb: at least 10-15 observations per predictor variable\nIncrease sample size if necessary to ensure stable coefficient estimates.\n\n\nAdditivity\nInteraction effects should be accounted for when necessary.\nTest for significant interaction terms.\nInclude interaction terms in the model if needed.\n\n\n\nViolating regression assumptions can distort results and lead to unreliable models. This is why diagnostic tools are essential in EDA to assess whether a model or its data is valid before blindly trusting it.\n\n\n\nThis handy guide visually breaks down the key assumptions of linear regression and what happens when they are violated.\n1) Linearity ensures that the relationship between variables follows a straight-line pattern; if a curved trend appears, consider transforming the variables.\n2) Homoscedasticity assumes that residuals have constant variance. When this is violated (as seen in the right image with a funnel shape), a non-linear transformation (linear-linear, linear-log, log-linear, log-log) can help.\n3) Multivariate normality requires residuals to be normally distributed for reliable statistical inference. If errors are skewed, try removing extreme outliers.\n4) Independence assumes that observations aren’t correlated over time (no autocorrelation). For time-series data, this often means incorporating lag terms or using time-series models like ARIMA. For non-time series data, look for clustered or grouped data to restructure, where observations within the same group are correlated.\n5) Multicollinearity occurs when predictors are highly correlated, which can distort coefficient estimates. This can be fixed by removing redundant variables, PCA, or applying lasso, ridge, or elastic net regression.\n6) While outliers aren’t strictly an assumption, they can heavily influence regression results; visualization tools like boxplots and influence diagnostics can help detect them.\n\n\nAt its simplest, regression estimates the relationship between an independent variable (predictor) and a dependent variable (outcome). The most common type is linear regression, which assumes a straight-line relationship:\n\\(Y = \\beta_0 + \\beta_1 X + \\varepsilon\\)\nWhere:\n\n\\(Y\\) = dependent variable (the response we want to predict)\n\\(X\\) = independent variable (the predictor)\n\\(\\beta_0\\) = intercept (the baseline value of \\(Y\\) when \\(X\\) is zero)\n\\(\\beta_1\\) = slope (how much \\(Y\\) changes for each unit increase in \\(X\\))\n\\(\\varepsilon\\) = error term (representing variability not explained by the model)\n\nRegression models draw a best-fit line through data points, aiming to get as close as possible to the actual values while minimizing the difference between predictions and reality. However, no model is perfect—there will always be some error. Residuals represent the gap between what a model predicts and what actually happens. To measure how well a model fits the data, regression minimizes the sum of squared residuals—meaning it finds the line that keeps errors as small as possible while avoiding overcomplicating the model. Squaring the residuals ensures that large mistakes are penalized more, leading to a more reliable and stable model.\n\n\n\nThis GIF is a handy visualization for understanding residuals and how regression minimizes the sum of squared error In Ordinary Least Squares (OLS) regression, the goal is to minimize the vertical distances between data points and the regression line, optimizing for prediction accuracy in the dependent variable. However, pay close attention to the gray lines in this animation—notice how they are perpendicular to the principal component line rather than vertical. This highlights the key difference between OLS regression and Principal Component Analysis (PCA): OLS minimizes vertical errors for prediction, while PCA minimizes orthogonal distances for finding the direction of maximum variance in the data. OLS is great for prediction tasks but assumes a clear dependent-independent variable relationship, while PCA excels in dimensionality reduction but isn’t designed for prediction.\n\n\nIn many real-world scenarios, a single predictor isn’t enough to explain an outcome. This is where multiple regression comes into play. Instead of modeling one independent variable (\\(X\\)), multiple regression incorporates multiple predictors (\\(X_n\\)):\n\\(Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + ... + \\beta_n X_n + \\varepsilon\\)\nSometimes, categorical variables (like “car type” or “region”) need to be included in a model. Since regression requires numerical inputs, we introduce dummy variables (indicator variables) to represent categories. For example, a dataset with “Sedan” and “Truck” cars could use a dummy variable where Sedan = 0 and Truck = 1. This allows regression models to incorporate qualitative factors and estimate their impact on the dependent variable.\nBeyond individual predictors, variables can interact with each other in meaningful ways. Interaction terms allow us to capture these effects. For instance, the effect of weight on CO₂ emissions might differ depending on whether a car is a sedan or truck. Instead of assuming weight has the same effect across all cars, an interaction term (i.e. Weight × Type) would account for this difference. Interaction terms can reveal complex relationships and reduce multi-collinearity.\nOther specialized forms of regression include:\n\nLogistic Regression: Used when the outcome is categorical (i.e. Yes/No, On/Off, Red/Blue/Yellow).\nPolynomial Regression: Captures non-linear relationships by fitting curves instead of straight lines.\nRidge, Lasso, and Elastic Net Regression: Address overfitting and improve predictive power by applying penalties to model complexity.\n\nNow, let’s return to our CO₂ emissions dataset. Intuitively, we expect larger vehicles to emit more CO₂, but how strong is that relationship? Using regression, we can quantify how much an increase in weight or horsepower influences emissions.\nWe define a linear regression model where CO₂ emissions (\\(Y\\)) are predicted based on multiple factors (\\(X_n\\)):\n\\(CO₂ = \\beta_0 + \\beta_1 \\times Weight + \\beta_2 \\times MPG + \\beta_3 \\times Model.Year + \\varepsilon\\)\nNow we’re ready to run the regression.\n\nmodel &lt;- lm(CO2 ~ MPG + Weight + Model.Year, data = data)\nsummary(model)\n\n\nCall:\nlm(formula = CO2 ~ MPG + Weight + Model.Year, data = data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-39.564 -14.362  -6.149   6.326 254.785 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  3.875e+03  1.952e+02   19.86   &lt;2e-16 ***\nMPG         -1.629e+01  3.359e-01  -48.49   &lt;2e-16 ***\nWeight       2.850e-02  2.084e-03   13.68   &lt;2e-16 ***\nModel.Year  -1.600e+00  1.040e-01  -15.39   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 25.17 on 1699 degrees of freedom\nMultiple R-squared:  0.9296,    Adjusted R-squared:  0.9295 \nF-statistic:  7482 on 3 and 1699 DF,  p-value: &lt; 2.2e-16\n\n\nThe regression results provide insights into the relationship between CO₂ emissions (g/mile) and three predictors: MPG (fuel efficiency), vehicle weight, and model year.\n\nThe negative coefficient for MPG (-1.629) indicates that for each additional mile per gallon a vehicle achieves, CO₂ emissions decrease by approximately 1.63 g/mile, which aligns with expectations—more fuel-efficient cars produce less CO₂.\nThe positive coefficient for weight (2.85) suggests that for every additional unit of vehicle weight, CO₂ emissions increase by about 2.85 g/mile, reinforcing the idea that heavier vehicles require more energy to move.\nThe model year coefficient (-1.60) suggests that for each newer model year, CO₂ emissions decrease by 1.6 g/mile, likely due to advancements in fuel efficiency and emissions regulations over time.\nThe intercept (3875 g/mile) represents the theoretical CO₂ emissions when all predictor variables are zero, though this isn’t a meaningful real-world scenario.\nAll predictors are highly statistically significant (p-values &lt; 2e-16), indicating strong evidence that these factors influence emissions.\nThe R-squared value (0.9296) suggests that the model explains about 92.96% of the variation in CO₂ emissions, meaning the chosen predictors provide a strong fit to the data.\nThe F-statistic (7482, p &lt; 2.2e-16) confirms that the overall model is highly significant.\n\nIn summary, this model confirms the expected trends—higher MPG and newer model years reduce emissions, while heavier vehicles contribute to increased CO₂ output. All done, right? Not quite. We did not look to see if the dataset violated the assumptions of regression. Let’s run a couple of visual diagnostics.\n\n\n\n\n\n\n\n\n\nUh oh, looks like our regression assumptions aren’t holding up well. The CO₂ vs. Model Year plot shows a nonlinear trend, violating the assumption of linearity. The CO₂ vs. MPG plot has a clear nonlinear pattern, suggesting a transformation (i.e. log transformation) could help linearize the relationship. The CO₂ vs. Weight plot shows increasing variance in residuals, indicating heteroscedasticity. The QQ plot of residuals reveals a heavy-tailed distribution, violating normality of residuals. Ideally, residuals should be normally distributed, homoscedastic, and independent, with a linear relationship between predictors and the outcome. These diagnostics helped us discover that the data by itself is not ready for modeling since it violates the assumptions of regression. To fix these issues, we can explore techniques shown in the table above, and also apply dummy variables and interaction terms."
  },
  {
    "objectID": "second-brain.html#anthropology",
    "href": "second-brain.html#anthropology",
    "title": "Second Brain",
    "section": "Anthropology",
    "text": "Anthropology\nHow did we get here? Homo sapiens have existed for nearly 300,000 years, yet the world we live in today—our cities, economies, technologies—has emerged only in a fraction of that time. For most of human history, we were hunter-gatherers, navigating the world in small, mobile groups. Civilization as we know it is a recent development, built on a series of transformative shifts that reshaped how we think, interact, and organize society. Anthropology examines these changes, uncovering patterns in human culture, behavior, and adaptation while posing a key question:\nHow does our evolutionary past still shape us today?\n\nFor most of human history, written records didn’t exist. This period is known as prehistory and was defined by survival adaptation and traditions passed down through generations. Early hominids like Neanderthals and Denisovans coexisted with Homo sapiens, but what set Homo sapiens apart was cooperation, imagination, and adaptability—traits that ultimately made them the only surviving species of the Homo genus.\nThe Cognitive Revolution (~70,000 years ago) was a key moment in this process, marking the emergence of abstract thought, symbolic language, and cultural belief systems, which laid the foundation for everything that followed.\n\n\n\n\n\nEach major shift in human history transformed how we lived:\n\nCognitive Revolution (~70,000 years ago): Expanded our ability to reason, plan, and organize in large groups.\nAgricultural Revolution (~10,000 years ago): Allowed humans to settle, grow food, and establish permanent communities, leading to population growth but also introducing social hierarchies, property disputes, and disease.\nSocial Revolution (~5,000 years ago): Formalized social structures through laws, religion, and economies, shaping governance and collective identity.\nScientific Revolution (~500 years ago): Fueled rapid technological advancements, transforming industries, global power structures, and our relationship with the environment.\n\nThese transitions didn’t happen overnight. No one at the time could have foreseen their impact, as each phase unfolded gradually over thousands of years, with both progress and unintended consequences.\nTo truly grasp the scale of these changes, explore the interactive treemap below. It visually represents how much time each period spans, offering a striking perspective on just how recent civilization, as we know it, really is.\n\n\n\n\n\n\n\nSo, why does this matter? Because our biological evolution hasn’t kept pace with the world we’ve created. The human brain evolved to function in small, close-knit communities, prioritizing immediate rewards and quick decision-making that were advantageous for survival. While these instinctual processes still influence behavior, the brain has also developed complex networks that support long-term planning, abstract reasoning, and technological innovation. These cognitive abilities allowed us to build civilizations, develop science, and expand our collective knowledge.\n\n\n\nHere’s a visual that demonstrates the different areas of the brain, dividing the brain into “reptilian,” “mammalian,” and “human” layers. This is called the triune brain model, but keep in mind that this model is outdated and is merely shown for learning sake. Modern neuroscience shows that the brain is a network of interconnected regions rather than isolated parts. While some areas are involved in instinct (brainstem), emotion (limbic system), and reasoning (neocortex), these functions are integrated rather than separate. The prefrontal cortex, amygdala, and other regions interact dynamically to regulate decision-making, emotions, and social behavior.\n\n\nThe tension between these two sides of the brain drives much of human behavior today. We are wired for rapid, emotionally charged responses driven by deep-seated survival mechanisms, yet we also engage in complex reasoning and problem-solving. This tension explains so much about human behavior: why we struggle with long-term decision-making, why fear and tribalism still drive much of our politics, and why expoential technologyical advancement can feel overwhelming to a species adapted for gradual change. No one is immune to the evolutionary forces that shaped us.\nUnderstanding where we came from helps us make sense of where we are now. Our instincts were shaped for a prehistoric world, yet we navigate an modern world of artificial intelligence, mass information, and rapid globalization. It’s as if we’re running on an outdated operating system, trying to process technological advancements that outpace our current cognitive evolution.\nIt took us tens of thousands of years to develop abstract thought, after all. Will we one day see another Cognitive Revolution, one that can handle today’s information input? Or is technology evolving too fast for us to keep up? If history has shown us anything, it’s that humans are capable of extraordinary transformation. The real question is: where do we go from here?"
  },
  {
    "objectID": "second-brain.html#history-religion",
    "href": "second-brain.html#history-religion",
    "title": "Second Brain",
    "section": "History & Religion",
    "text": "History & Religion\nHistory is often divided into broad periods based on technological advancements, societal organization, and dominant ideologies. The general progression follows:\n\nStone Age → Ancient → Medieval → Modern\n\nIn historical sequence:\n\nPrehistory → Ancient Greece & the Roman Empire → Feudalism & Religion (Dark Ages) → Enlightenment → Globalization & Science\n\nThese transitions were influenced by factors such as Marx’s theory of history and dialectical materialism, which frames history as a struggle between economic classes, and political evolution from tribes → kingdoms → empires.\nThe table below outlines major historical ages, their defining characteristics, and key events that shaped human civilization.\n\n\n\n\n\nAge\nName of Era\nYear Period\n# of Years\nMain Event\nDetails of Events\n\n\n\n\nStone\nPaleolithic\n3,000,000 - 10,000 BCE\n2,990,000\nHomo sapien evolution and the Ice Age\nUse of stone tools, development of language, early art\n\n\nStone\nMesolithic\n10,000 - 8,000 BCE\n2,000\nTransition to farming\nDomestication of animals, microlithic tools, early settlements\n\n\nStone\nNeolithic\n8,000 - 3,000 BCE\n5,000\nAgricultural revolution\nPermanent settlements, pottery, weaving, advanced tools\n\n\nAncient\nBronze Age\n3,000 - 1,200 BCE\n1,800\nWriting and structured civilization\nDevelopment of writing systems, organized civilization, trade networks\n\n\nAncient\nIron Age\n1,200 - 500 BCE\n700\nRise of empires\nWidespread iron use, territorial expansion, centralized rule\n\n\nAncient\nClassical Antiquity\n500 BCE - 500 CE\n1,000\nExpansion and political structures\nUrbanization, philosophy, governance, large-scale conflicts\n\n\nMedieval\nEarly Middle Ages\n500 - 1000 CE\n500\nDecentralization and rise of feudalism\nLocal rule, decline of urban centers, self-sufficient economies\n\n\nMedieval\nHigh Middle Ages\n1000 - 1300 CE\n300\nCrusades and scholasticism\nReligious control over society, universities, increased trade\n\n\nMedieval\nLate Middle Ages\n1300 - 1500 CE\n200\nCrisis and early Enlightenment thought\nBlack Death, political unrest, scientific exploration, early Enlightenment\n\n\nModern\nEarly Modern Period\n1500 - 1800 CE\n300\nRenaissance and colonization\nGlobal exploration, reformation, scientific advancements\n\n\nModern\nIndustrial Age\n1800 - 1950 CE\n150\nIndustrialization and world wars\nFactories, steam power, electrification, world wars\n\n\nModern\nContemporary Era\n1950 - Present\n70\nDigital and globalized information\nComputers, internet, space exploration, globalization\n\n\n\n\n\nEach period represents shifts in economic structures, governance, and technological innovations. The transition from hunter-gatherer societies in the Paleolithic Age to agrarian communities in the Neolithic Age set the foundation for the rise of civilizations. The Bronze and Iron Ages introduced writing, trade networks, and structured governance, leading to Classical Antiquity, which saw the emergence of philosophy, law, and empire-building.\nThe Medieval Period marked decentralization following the collapse of the Roman Empire, with feudalism dominating Europe. The Renaissance and Early Modern Period spurred exploration, scientific thought, and global interactions, leading into the Industrial Age, where mechanization and world wars reshaped human progress. Finally, the Contemporary Era is defined by digital globalization and rapid technological advancements.\nWhile broad historical ages provide structure, civilizations themselves had distinct trajectories. Some expanded through conquest, others thrived through trade, and many fell due to internal decline or external pressures. The chart below visualizes major civilizations across different regions and their timelines.\n\n\n\n\n\n\nFrom the ancient cities of Mesopotamia and Egypt to the global empires of modern times, each civilization adapted to its environment and left lasting cultural, political, and technological legacies. The decline of empires often coincided with shifts in governance, economic instability, or military conflicts.\nReligion has played a fundamental role in shaping human history, often driving social cohesion, political authority, and cultural identity. Many empires justified their rule through divine mandate, while religious movements have both unified and fractured societies.\nThe timeline below illustrates the emergence of major world religions and their regions of origin.\n\n\n\n\n\n\nFrom polytheistic traditions in ancient civilizations to the rise of monotheistic faiths and their global influence, religion has shaped governance, philosophy, and societal norms. The dominance of certain religious traditions in specific eras often coincided with the rise of corresponding empires.\nLooking at long-term trends, several patterns emerge:\n\nThe Dark Ages and Intellectual Revival: Following the collapse of the Roman Empire, Europe entered a period of feudal fragmentation. However, the Islamic Golden Age preserved and expanded knowledge, influencing the later European Renaissance.\nEurocentric Historical Narratives: While dominant global narratives often focus on Europe’s progression, civilizations in Asia, Africa, and the Americas played equally vital roles in shaping human history.\nCyclical Patterns: Some historians suggest that history follows cyclical patterns, such as the Strauss-Howe generational theory, which proposes societal shifts occurring approximately every 250 years.\n\nUnderstanding history through these frameworks allows us to recognize past patterns, appreciate diverse contributions, and anticipate potential future shifts.\nto include: - rites, rituals, community -&gt; very human - more high lvl look at religion origins"
  },
  {
    "objectID": "second-brain.html#philosophy",
    "href": "second-brain.html#philosophy",
    "title": "Second Brain",
    "section": "Philosophy",
    "text": "Philosophy\nUnder Construction\n\nSocrates\nPlato\nAristotle (bring up catharsis; more on this in sociology)\nhedonism vs. cynic vs. stoic\nDescartes"
  },
  {
    "objectID": "second-brain.html#sociology",
    "href": "second-brain.html#sociology",
    "title": "Second Brain",
    "section": "Sociology",
    "text": "Sociology\nUnder Construction\n\nthe medium is the message\nthe culture industry\nreclaiming conversations\nbowling alone\namusing ourselves to death"
  },
  {
    "objectID": "second-brain.html#psychology",
    "href": "second-brain.html#psychology",
    "title": "Second Brain",
    "section": "Psychology",
    "text": "Psychology\nUnder Construction\n\nhistory: freud, jung, Edward Bernays, etc\npersonal trauma\ncultural trauma (advertising, hustle culture, honor)\npersonality disorders\nrational vs emotional mind vs wise mind (DBT)\ndifferent therapy modalities"
  },
  {
    "objectID": "portfolio-artistic.html",
    "href": "portfolio-artistic.html",
    "title": "Artistic Portfolio",
    "section": "",
    "text": "Below are selections from my artistic portfolio, including fiction, poetry, spoken word, dance, photography, songwriting, music performance, and visual arts. My strongest work is in fiction and poetry, though many of those pieces are reserved for future publication."
  },
  {
    "objectID": "portfolio-artistic.html#fiction",
    "href": "portfolio-artistic.html#fiction",
    "title": "Artistic Portfolio",
    "section": "Fiction",
    "text": "Fiction\nThe Serpent’s Utopia, published in a local anthology on the theme of crossroads, is a spoken-word short story inspired by Genesis that explores how growth emerges only when we embrace discomfort and risk.\nThis piece was written to be heard. Press play to listen to the recorded performance.   Your browser does not support the audio element."
  },
  {
    "objectID": "portfolio-artistic.html#poetry",
    "href": "portfolio-artistic.html#poetry",
    "title": "Artistic Portfolio",
    "section": "Poetry",
    "text": "Poetry\nMy Phony Friend \nThe Violinist’s Ballad \nOutside The Box \nSorella Gelateria \n\n\nClick for more details:\n\nWhat an incredible joy it is to gift poem & art to my favorite small business. I adore the summers filled with bike rides to the local gelateria run by two sisters. The love and tears received warmed my heart.\n\n\n\n\n\n\nHistory of Humanity"
  },
  {
    "objectID": "portfolio-artistic.html#spoken-word",
    "href": "portfolio-artistic.html#spoken-word",
    "title": "Artistic Portfolio",
    "section": "Spoken Word",
    "text": "Spoken Word\n\n\n\nYour browser does not support the video tag.\n\n\n\nYour browser does not support the video tag."
  },
  {
    "objectID": "portfolio-artistic.html#dance",
    "href": "portfolio-artistic.html#dance",
    "title": "Artistic Portfolio",
    "section": "Dance",
    "text": "Dance\nMy partner and I won third place in a Salsa Competition with our very first performance and original choreography. I usually dance socially with improvised moves and musicality, but I really enjoyed leaning into the story and emotion for this piece.\n\n\nYour browser does not support the video tag."
  },
  {
    "objectID": "portfolio-artistic.html#photography",
    "href": "portfolio-artistic.html#photography",
    "title": "Artistic Portfolio",
    "section": "Photography",
    "text": "Photography"
  },
  {
    "objectID": "portfolio-artistic.html#songwriting",
    "href": "portfolio-artistic.html#songwriting",
    "title": "Artistic Portfolio",
    "section": "Songwriting",
    "text": "Songwriting\nAll songs were created in 2022 to explore lyric and melody.    Your browser does not support the video tag.     Your browser does not support the video tag.  \n\n\nClick for more details:\n\nThis song began at a Poetry On Demand stand in Asheville, NC. I shared a few words, and the poet wove them into a poem. I loved it so much that I made it into a song. The original poem is included below. \n\n   Your browser does not support the video tag.     Your browser does not support the video tag.  \n\n\nClick for more details:\n\nThis song came from a playful challenge: I asked a friend for random words to make them into lyrics. The chosen words were:\n\nPomegranates\nBlue bonnets\nSweater vest\nSunset\nGeorgia\nWagon wheel\n\n\n\n\n\nYour browser does not support the video tag.  \n\n\nClick for more details:\n\nThis song came from a playful challenge: I asked my partner for random words to make them into lyrics. The chosen words were:\n\nCotton candy\nTwix\nPenguin\nNew York City\nIce skating"
  },
  {
    "objectID": "portfolio-artistic.html#violin",
    "href": "portfolio-artistic.html#violin",
    "title": "Artistic Portfolio",
    "section": "Violin",
    "text": "Violin\nAlthough I no longer play, violin was once a deep passion of mine. Below are videos from when I was 17/18.\nViolin Looper Performance of “Secrets” by OneRepublic, in the style of Bryson Andres   Your browser does not support the video tag. \nChamber Orchestra Performance of my arrangement to Lana Del Rey’s “Young And Beautiful”   Your browser does not support the video tag. \nViolin & Cello Duet Practice (unfortunately, I no longer have the composer’s name)   Your browser does not support the audio element."
  },
  {
    "objectID": "portfolio-artistic.html#visual-art",
    "href": "portfolio-artistic.html#visual-art",
    "title": "Artistic Portfolio",
    "section": "Visual Art",
    "text": "Visual Art\nThis garage mural was a collaboration between my friend, her husband, my sister, and me. It has since become a neighborhood landmark, bringing smiles to those who pass by."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome!",
    "section": "",
    "text": "Bridging Data, Community, and Creativity\nHey there! I’m Carina Grady—an engineer, data analyst, and community advocate who loves bringing people and ideas together. My work blends technical expertise with creative expression and civic engagement, whether I’m analyzing data to support my client projects, brainstorming ways to strengthen my city’s local businesses, or engaging in deep discussions with my local community.\nI believe data can tell powerful stories, but so can movement, words, and human connection. That’s why, outside of my work in consulting and analytics, you’ll find me immersed in the arts—writing, reading, or attending events that bring my city to life. I love a good deep-dive into analytics just as much as I love a lively dance floor or a compelling poem.\nThis site is a reflection of all those things: data, creativity, and community. Take a look around, and if something sparks an idea, let’s connect!\n\nThis site is built entirely with Quarto and R markdown! View the source code on GitHub."
  },
  {
    "objectID": "contact.html",
    "href": "contact.html",
    "title": "Contact",
    "section": "",
    "text": "Have a question or want to connect? Fill out the form below, and I’ll get back to you!\n\nName: \nEmail: \nMessage:\n\n\nSend Message"
  }
]